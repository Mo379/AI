# -*- coding: utf-8 -*-
"""Convnets_jax.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CielWElRDbMRdw3sezwNTn6HhYjrF8wV
"""

import jax
import jax.numpy as jnp
from jax.example_libraries import stax, optimizers
import operator as op
import functools
#
import numpy as np
import matplotlib.pyplot as plt
import functools
from torchvision.datasets import MNIST

#
print("JAX Version : {}".format(jax.__version__))

def custom_transform(image):
  image = np.ravel(np.array(image, dtype=np.float32))
  return image
mnist_train = MNIST(root='./data', train=True, download=True, transform=custom_transform)
mnist_test = MNIST(root='./data', train=False, download=True, transform=custom_transform)
#fetch and reshape training
mnist_train_features = jnp.array(mnist_train.data).reshape(-1,28,28,1)
mnist_train_labels = jnp.array(mnist_train.targets)
classes =  jnp.unique(mnist_train_labels)
#fetch and reshape testing
mnist_test_features = jnp.array(mnist_test.data).reshape(-1,28,28,1)
mnist_test_labels = jnp.array(mnist_test.targets)
#normalise
mnist_train_features, mnist_test_features = mnist_train_features/255.0, mnist_test_features/255.0
mnist_train_features.shape

#my libarary
def my_combinator(*layers):
  n_layers = len(layers)
  init_funs, apply_funs = zip(*layers)
  def init_fn(rng, input_shape):
    params = []
    for init_fun in init_funs:
      rng, layer_rng = jax.random.split(rng)
      input_shape, layer_params = init_fun(layer_rng, input_shape)#init funs return output-shape, params 
      params.append(layer_params)
    return input_shape, params
  def apply_fn(layers_params, inputs, **kwargs):
    rng = kwargs.pop('rng', None)
    rngs = jax.random.split(rng, n_layers) if rng is not None else (None,) * n_layers
    for apply_fun,params,rng in zip(apply_funs,layers_params,rngs):
      inputs = apply_fun(params,inputs, **kwargs)
    return inputs
  return init_fn, apply_fn
def element_wise(function, **fun_kwargs):
  init_fun = lambda rng,input_shape: (input_shape,())
  apply_fun = lambda params,inputs, **kwargs: function(inputs, **fun_kwargs)
  return init_fun, apply_fun
Relu_layer = element_wise(jax.nn.relu)
Softmax_layer = element_wise(jax.nn.softmax, axis=-1)
golrot = jax.nn.initializers.glorot_normal
normal = jax.nn.initializers.normal
def my_Dense(out_dim, w_init = golrot(), b_init = normal()):
  def init_fn(rng, input_shape):
    output_shape = input_shape[:-1] + (out_dim,)
    k_w, k_b = jax.random.split(rng)
    w, b = w_init(k_w,(input_shape[-1], out_dim)),b_init(k_b,(out_dim,))
    params = (w,b)
    return output_shape, params
  def apply_fn(params, inputs, **kwargs):
    w,b = params
    return jnp.dot(inputs,w) + b
  return init_fn, apply_fn
def my_Flatten():
  def init_fn(rng, input_shape):
    output_shape = input_shape[0], functools.reduce(op.mul, input_shape[1:])
    return output_shape,  ()
  def apply_fn(params, inputs, **kwargs):
    return jnp.reshape(inputs, (inputs.shape[0], -1))
  return init_fn, apply_fn

#stack layers
init_fun, apply_fun = my_combinator(
    stax.Conv(32,(3,3), padding='SAME'),Relu_layer,
    stax.Conv(32,(5,5), padding='SAME'),Relu_layer,
    stax.Conv(32, (5,5),padding='SAME'),Relu_layer,
    my_Flatten(),
    my_Dense(10),Softmax_layer
)

#init params
seed = 0
rng = jax.random.PRNGKey(seed)
batch_size = 32
out_shape, params = init_fun(rng, (batch_size, 28,28,1))
jax.tree_map(lambda x: x.shape, params)

#make sample batch predictions
batch_X, batch_Y = mnist_train_features[0:batch_size],mnist_train_labels[0:batch_size] 
apply_fun(params, batch_X)[0]

#make binary cross entrpy loss and test on sample batch
def CrossEntropyLoss(params, x,y):
  prds = apply_fun(params,x)
  log_prds = jnp.log(prds)
  one_hot = jax.nn.one_hot(y, num_classes = len(classes))
  return - jnp.sum(one_hot * log_prds)
CrossEntropyLoss(params,batch_X,batch_Y)

def TrainModelInBatches(X,Y,epochs,opt_state,batch_size=32):
  for i in range(1,epochs+1):
    batches = jnp.arange((X.shape[0]//batch_size)+1)
    losses = []
    for batch in batches:
      if batch != batches[-1]:
        start, end = int(batch*batch_size), int(batch*batch_size + batch_size)
      else:
        start, end = int(batch*batch_size), None
      X_batch,Y_batch = X[start:end], Y[start:end]

      loss, grads = jax.value_and_grad(CrossEntropyLoss)(opt_get_params(opt_state), X_batch,Y_batch)
      opt_state = opt_update(i, grads, opt_state)
      losses.append(loss)
    print("CrossEntropyLoss : {:.3f}".format(jnp.array(losses).mean()))
  return opt_state

# compose everything from the start
seed = 0
key = jax.random.PRNGKey(seed)
learning_rate = 0.01
epochs = 100
batch_size = 256
shape_params = init_fun(key, (batch_size,28,28,1))
params = shape_params[1]

opt_init, opt_update, opt_get_params = optimizers.adagrad(learning_rate)
opt_state = opt_init(params)

X = mnist_train_features 
Y = mnist_train_labels 

CrossEntropyLoss = jax.jit(CrossEntropyLoss)
opt_update = jax.jit(opt_update)
opt_get_params = jax.jit(opt_get_params)
final_state = TrainModelInBatches(X,Y,epochs,opt_state,batch_size=batch_size)

