# -*- coding: utf-8 -*-
"""XOR_in_jax.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KVbwLvG4t5foOsllwY4m61hV-SJH9sZm
"""

import jax
import jax.numpy as jnp
import numpy as np
import matplotlib.pyplot as plt
import functools
import jax.tools.colab_tpu
from torchvision.datasets import MNIST

"""# The problem"""

X = jnp.array([[0,0],[0,1],[1,0],[1,1]])
Y = jnp.array([[0],[1],[1],[0]])
X.shape,Y.shape

seed = 0
key = jax.random.PRNGKey(seed)

def init_model(model_shape,parent_key,scale=0.01):
  params = []
  keys = jax.random.split(parent_key, num = len(model_shape)-1)

  for in_dim, out_dim, key in zip(model_shape[:-1], model_shape[1:],keys):
    w_key, b_key = jax.random.split(key)
    initializer = jax.nn.initializers.glorot_normal()
    weights = scale*initializer(w_key, shape=(out_dim,in_dim))
    biases = scale*jax.random.normal(b_key, shape=(out_dim,))
    #

    #
    params.append(
        [
         weights,
         biases,
        ]
    )
  return params

def relu(preactivations):
  return jnp.maximum(preactivations,0)

def forward(params,x, la=0.01):
  d_out = jax.tree_map(lambda x: (x*0) + np.random.binomial(size=x.shape, n=1, p= 0.9) , params)
  *hidden, last = params
  *d_h, d_l = d_out

  for layer,d_prob in zip(hidden,d_h):
    x = relu(jnp.dot(layer[0]*d_prob[0],x) + layer[1]) + la*jnp.sum(layer[0]**2)
  return jnp.dot(last[0],x) + last[1] + la*jnp.sum(last[0]**2)
batch_forward = jax.vmap(forward, in_axes = (None,0))

def loss_fn(params,x,y):
  #MSE_LOSS
  return (1/len(x))*jnp.sum((batch_forward(params,x)-y)**2)

def sgd(params,grad,lr=0.01):
  return jax.tree_multimap(lambda p,g: p - lr*g,params,grad)

def sgd_momentum(params,grad,alpha=0.01,gamma=0.001):
  momentum = jax.tree_map(lambda g: gamma - alpha*g, grad )
  return jax.tree_multimap(lambda p,mom: p + mom ,params,momentum)

def rms_prop(params,grad,r_,alpha=0.001,rho=0.9,epsilon=10**(-7)):
  r_t = jax.tree_multimap(lambda g,r: rho*r +(1-rho)*g**(2) ,grad,r_)
  w_t = jax.tree_multimap(lambda g,r,p: p - alpha*(1/(jnp.sqrt(r)+epsilon))*g,grad,r_t,params)
  return w_t,r_t

def adam(params,grad,s_,r_,B1_=0.9,B2_=0.999,alpha=0.01,epsilon=10**(-7)):
  s_t = jax.tree_multimap(lambda s,g: B1_*s + (1-B1_)*g ,s_,grad)
  r_t = jax.tree_multimap(lambda r,g: B2_*r + (1-B2_)*g**2  ,r_,grad)
  
  s_hat_t = jax.tree_map(lambda s: s/(1-B1_), s_t)
  r_hat_t = jax.tree_map(lambda r: r/(1-B2_), r_t)
  w_t = jax.tree_map(lambda w,r,s: w -((alpha)*(1/(jnp.sqrt(r+epsilon)))*(s)),params,r_hat_t,s_hat_t)
  return w_t,s_t,r_t
@jax.jit
def update(params,x,y,s_t,r_t):
  loss,grad = jax.value_and_grad(loss_fn)(params,x,y)
  #
  #params = sgd(params,grad)
  #params = sgd_momentum(params,grad)
  #params,r_t = rms_prop(params,grad,r_t)
  params,s_t, r_t = adam(params,grad,s_t,r_t)
  return params,loss,s_t,r_t

import time
l = [2,1]
add = [2]*1
shape = l[:len(l)//2] + add + l[len(l)//2:]
params = init_model(shape, key)
n_epochs = 10000
start = time.time()
s_t = jax.tree_map(lambda x: x-x , params)
r_t = jax.tree_map(lambda x: x-x , params)
for epoch in range(n_epochs):
  params,loss,s_t,r_t = update(params,X,Y,s_t,r_t)
  if epoch % int(n_epochs/10) == 0 :
    print(loss)
end = time.time()
print(f"total_time: {end-start}")

np.random.rand()



